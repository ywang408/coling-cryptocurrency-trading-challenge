{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Tutorial for Agent-based Crypto Trading Challenge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Agent-based Single Cryptocurrency Trading Challenge](https://coling2025cryptotrading.thefin.ai/), we ask participants to submit a pre-trained/finetuned model for cryptocurrency trading scenario . The submitted model will be used as the backbone model to be tested for performance under the Finmem - an agent-based trading framework. We hope to explore the performance of open source LLMs as the backbones in agent framework on trading tasks. In this tutorial, we use last [LLM challenge @ IJCAI 2024](https://huggingface.co/docs/peft/en/conceptual_guides/adapter) as an example to show how to fine-tuning your specific model. <br>\n",
    "**Note: You can use any data you find helpful for fine-tuning. If you have sufficient computing resource and data, you can also pre-train model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">\n",
    "    <p>Pre-knowledge: Parameter-Efficient Fine-Tuning (PEFT) methods</p>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-training or full-precision fine-tuning large pretrained models is often prohibitively costly due to their scale. Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of large pretrained models to various downstream applications by only fine-tuning a small number of (extra) model parameters instead of all the model's parameters. This significantly decreases the computational and storage costs. PEFT is integrated with Transformers for easy model training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!TIP]\n",
    "> Visit the [PEFT Githug Repo](https://github.com/huggingface/peft) to read about the more PEFT example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!TIP]\n",
    "> Visit the [PEFT](https://huggingface.co/PEFT) organization to read about the PEFT methods implemented in the library and to see notebooks demonstrating how to apply these methods to a variety of downstream tasks. Click the \"Watch repos\" button on the organization page to be notified of newly implemented methods and notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use PEFT in your project, please cite it by using the following BibTeX entry.\n",
    "\n",
    "```bibtex\n",
    "@Misc{peft,\n",
    "  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},\n",
    "  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},\n",
    "  howpublished = {\\url{https://github.com/huggingface/peft}},\n",
    "  year =         {2022}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation \n",
    "\n",
    "Install PEFT from pip:\n",
    "\n",
    "```bash\n",
    "pip install peft\n",
    "```\n",
    "\n",
    "Other requirement packages:\n",
    "```bash\n",
    "transformers\n",
    "accelerate\n",
    "bitstandbytes\n",
    "flash-attn\n",
    "huggingface-hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Example\n",
    "import necessary package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from random import randrange, sample, seed\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "from utils import concatenate_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT Configration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfigurator:\n",
    "    def __init__(self, model_id, output_dir, train_dataset, use_flash_attention2):\n",
    "        self.model_id = model_id\n",
    "        self.output_dir = output_dir\n",
    "        self.train_data = train_dataset\n",
    "        self.use_flash_attention2 = use_flash_attention2\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "    \n",
    "    # Quantization config: 8-bit quantization / 4-bit quantization can help you save GPT memory usage in fine-tuning and inference\n",
    "    def bit_config(self):\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16 if self.use_flash_attention2 else torch.float16\n",
    "        )\n",
    "    \n",
    "    # Load model and tokenizer for base model\n",
    "    def load_model_and_tokenizer(self):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id, \n",
    "            quantization_config=self.bit_config(), \n",
    "            use_cache=False, \n",
    "            device_map=\"auto\",\n",
    "            token = \"\", \n",
    "            attn_implementation=\"flash_attention_2\" if self.use_flash_attention2 else \"sdpa\"\n",
    "        )\n",
    "        self.model.config.pretraining_tp = 1\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_id,\n",
    "            token = \"\", # Put your huggingface token here \n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        return self.model, self.tokenizer\n",
    "\n",
    "    # PEFT config\n",
    "    def peft_config(self):\n",
    "        peft_config = LoraConfig(\n",
    "                lora_alpha=16,\n",
    "                lora_dropout=0.1,\n",
    "                r=8,\n",
    "                bias=\"none\",\n",
    "                task_type=\"CAUSAL_LM\",\n",
    "                target_modules=[\n",
    "                    \"q_proj\",\n",
    "                    \"k_proj\",\n",
    "                    \"v_proj\",\n",
    "                    \"o_proj\",\n",
    "                    \"gate_proj\", \n",
    "                    \"up_proj\", \n",
    "                    \"down_proj\",\n",
    "                ]\n",
    "        )\n",
    "        return peft_config\n",
    "    \n",
    "    # Trainer config\n",
    "    def trainer_config(self):\n",
    "        args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=1,\n",
    "            per_device_train_batch_size=6 if self.use_flash_attention2 else 2, # you can play with the batch size depending on your hardware\n",
    "            gradient_accumulation_steps=4,\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=2e-4,\n",
    "            bf16=self.use_flash_attention2,\n",
    "            fp16=not self.use_flash_attention2,\n",
    "            tf32=self.use_flash_attention2,\n",
    "            max_grad_norm=0.3,\n",
    "            warmup_steps=5,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            disable_tqdm=False,\n",
    "            report_to=\"none\"\n",
    "            )   \n",
    "        return args\n",
    "    \n",
    "    # Train the model\n",
    "    def train_model(self):\n",
    "        model, tokenizer = self.load_model_and_tokenizer()\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, self.peft_config())\n",
    "\n",
    "        self.trainer = SFTTrainer(\n",
    "                model=model,\n",
    "                train_dataset=self.train_data,\n",
    "                dataset_text_field=\"text\",\n",
    "                peft_config=self.peft_config(),\n",
    "                max_seq_length=2048,\n",
    "                tokenizer=tokenizer,\n",
    "                packing=True,\n",
    "                # formatting_func=format_instruction, \n",
    "                args=self.trainer_config(),\n",
    "                )\n",
    "        self.trainer.train() # Staring fine-tuning\n",
    "        self.trainer.save_model() # Save the model in your local disk. You can also upload it to huggingface hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction-follwing dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def concatenate_fields(dataset: Dataset) -> Dataset:\n",
    "    def concat_example(example):\n",
    "        concatenated_text = r\"<s>[INST] \" + example['query'] + r\" [/INST] \" + example['answer'] + r\" </s>\"\n",
    "        return {'text': concatenated_text}\n",
    "\n",
    "    new_dataset = dataset.map(concat_example)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface login\n",
    "login(token = '')\n",
    "\n",
    "\n",
    "\"\"\" parameters setting \"\"\"\n",
    "seed(42)\n",
    "task_tune = \"task1\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\" # Fine-tuning on Llama-3-8B-Instruct\n",
    "#model_id = \"mistralai/Mistral-7B-Instruct-v0.2\" # Fine-tuning on Mistral-3-8B-Instruct\n",
    "output_dir = f\"llama3-8B-int4-{task_tune}\"\n",
    "\n",
    "use_flash_attention2 = False\n",
    "# Replace attention with flash attention \n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    use_flash_attention2 = True\n",
    "print(f\"Using flash attention 2: {use_flash_attention2}\")\n",
    "\n",
    "\n",
    "\"\"\" dataset prepare and train/val split \"\"\"\n",
    "if task_tune == \"task1\":\n",
    "    dataset1 = load_dataset(\n",
    "    \"TheFinAI/finarg-ecc-auc_train\", \n",
    "    split=\"train\", \n",
    "    token=\"\"\n",
    "    )\n",
    "    dataset = concatenate_fields(dataset1)\n",
    "\n",
    "elif task_tune == \"task2\":\n",
    "    dataset2 = load_dataset(\n",
    "    \"TheFinAI/edtsum_train\", \n",
    "    split=\"train\", \n",
    "    token=\"\"\n",
    "    )\n",
    "    dataset = concatenate_fields(dataset2)\n",
    "\n",
    "else: # data augmentation \n",
    "      # In last challenge, we take data fusion strategy to fine-tune the model by putting the data of task1 and task2 together\n",
    "      # to improve the fine-tuning performance.\n",
    "    dataset1 = load_dataset(\n",
    "    \"TheFinAI/finarg-ecc-auc_train\", \n",
    "    split=\"train\", \n",
    "    token=\"\"\n",
    "    )\n",
    "    dataset2 = load_dataset(\n",
    "    \"TheFinAI/edtsum_train\", \n",
    "    split=\"train\", \n",
    "    token=\"\"\n",
    "    )\n",
    "    dataset1 = concatenate_fields(dataset1)\n",
    "    dataset2 = concatenate_fields(dataset2)\n",
    "    dataset = concatenate_datasets([dataset1, dataset2])\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "if task_tune == \"task1\":\n",
    "    n_samples = sample(range(len(dataset)), k=6200)\n",
    "elif task_tune == \"task2\":\n",
    "    n_samples = sample(range(len(dataset)), k=6400)\n",
    "else:\n",
    "    n_samples = sample(range(len(dataset)), k=12500)\n",
    "print(f\"First 5 samples: {n_samples[:5]}\")\n",
    "train_dataset = dataset.select(n_samples)\n",
    "print(f\"Reduced dataset size: {len(dataset)}\")\n",
    "all_indices = set(range(len(dataset)))\n",
    "validation_indices = list(all_indices - set(n_samples))\n",
    "validation_dataset = dataset1.select(validation_indices)\n",
    "print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
    "validation_df = validation_dataset.to_pandas()\n",
    "validation_df.to_csv(f'validation_set_{task_tune}_trail_1.csv', index=False)\n",
    "\n",
    "\n",
    "\"\"\" fine-tuning \"\"\"\n",
    "configurator = ModelConfigurator(model_id, output_dir, train_dataset, use_flash_attention2)\n",
    "configurator.train_model()\n",
    "\n",
    "print(\"Finetuning completed!\")\n",
    "\n",
    "\"\"\" uploading your model to huggingface hub \"\"\"\n",
    "#model.push_to_hub(\"your-hf-username/my-awesome-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once you upload your model to huggingface hub, you can submit the link to the challenge organizer. The organizer will access your model from the provided link**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citing \n",
    "\n",
    "If you find the tutorial useful, please citte\n",
    "```bibtex\n",
    "@inproceedings{cao2024catmemo,\n",
    "  title={CatMemo at the FinLLM Challenge Task: Fine-Tuning Large Language Models using Data Fusion in Financial Applications},\n",
    "  author={Cao, Yupeng and Yao, Zhiyuan and Chen, Zhi and Deng, Zhiyang},\n",
    "  booktitle={Joint Workshop of the 8th Financial Technology and Natural Language Processing (FinNLP) and the 1st Agent AI for Scenario Planning (AgentScen) in conjunction with IJCAI 2023},\n",
    "  pages={174},\n",
    "  year={2024}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
